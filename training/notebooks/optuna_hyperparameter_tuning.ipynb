{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "J2-D0HjBpNwm",
        "outputId": "339727bb-9d23-4574-be29-9225831b318f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.7.14)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1DB-r714_Wr2J4boXFuvOHn6hUwUO3fgR\n",
            "From (redirected): https://drive.google.com/uc?id=1DB-r714_Wr2J4boXFuvOHn6hUwUO3fgR&confirm=t&uuid=0fe41eb4-04dd-436c-954c-54ba7b5a9f72\n",
            "To: /content/datasets.zip\n",
            "100% 107M/107M [00:04<00:00, 22.3MB/s]\n",
            "Archive:  /content/datasets.zip\n",
            "replace /content/datasets/ants/ants (1).jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!pip install -U gdown\n",
        "\n",
        "# Download the zip file\n",
        "!gdown --id 1DB-r714_Wr2J4boXFuvOHn6hUwUO3fgR -O datasets.zip\n",
        "!unzip /content/datasets.zip -d /content/datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision\n",
        "!pip install tqdm\n",
        "!pip install matplotlib seaborn scikit-learn pillow\n",
        "!pip install optuna\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uTQ44f21uuh4",
        "outputId": "a8c26e4d-5186-4eb4-a537-9d6a90595f5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.4.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.16.4)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OPTUNA Hypertunning"
      ],
      "metadata": {
        "id": "AsgYX5U7u8e5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from torchvision import transforms, models\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import optuna\n",
        "\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "\n",
        "def get_classifier(num_layers, in_features, out_features, dropout_rate, hidden_units, activation_name):\n",
        "    \"\"\"\n",
        "    Dynamically build classifier head with given hyperparameters\n",
        "    \"\"\"\n",
        "    layers = []\n",
        "    input_dim = in_features\n",
        "    activation_fn = {\n",
        "        'relu': nn.ReLU(inplace=True),\n",
        "        'leaky_relu': nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
        "    }[activation_name]\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        layers.append(nn.Linear(input_dim, hidden_units))\n",
        "        layers.append(activation_fn)\n",
        "        layers.append(nn.Dropout(dropout_rate))\n",
        "        input_dim = hidden_units\n",
        "    layers.append(nn.Linear(input_dim, out_features))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def get_scheduler(optimizer, scheduler_name, patience):\n",
        "    if scheduler_name == 'ReduceLROnPlateau':\n",
        "        return torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='min', factor=0.5, patience=patience, verbose=True)\n",
        "    elif scheduler_name == 'StepLR':\n",
        "        return torch.optim.lr_scheduler.StepLR(optimizer, step_size=patience, gamma=0.5)\n",
        "    elif scheduler_name == 'CosineAnnealingLR':\n",
        "        return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=patience*10)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "    # Hyperparameters to tune\n",
        "    lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n",
        "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)\n",
        "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
        "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'AdamW', 'SGD'])\n",
        "    num_layers = trial.suggest_int('num_classifier_layers', 1, 3)\n",
        "    hidden_units = trial.suggest_categorical('hidden_units', [128, 256, 512])\n",
        "    activation_name = trial.suggest_categorical('activation', ['relu', 'leaky_relu'])\n",
        "    scheduler_name = trial.suggest_categorical('scheduler', ['ReduceLROnPlateau', 'StepLR', 'CosineAnnealingLR', 'None'])\n",
        "    scheduler_patience = trial.suggest_int('scheduler_patience', 3, 10)\n",
        "    rotation_degree = trial.suggest_float('rotation_degree', 10.0, 40.0)\n",
        "    brightness = trial.suggest_float('brightness', 0.1, 0.5)\n",
        "    contrast = trial.suggest_float('contrast', 0.1, 0.5)\n",
        "    saturation = trial.suggest_float('saturation', 0.1, 0.5)\n",
        "    hue = trial.suggest_float('hue', 0.01, 0.15)\n",
        "    num_epochs = trial.suggest_int('num_epochs', 2, 6)\n",
        "    # For early stopping/pruning\n",
        "    early_stopping_patience = trial.suggest_int('early_stopping_patience', 3, 7)\n",
        "\n",
        "    # Setup augmentations reflecting the hyperparameters\n",
        "    augmentations = transforms.Compose([\n",
        "        transforms.Resize((224 + 32, 224 + 32)),\n",
        "        transforms.RandomCrop(224),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomVerticalFlip(p=0.3),\n",
        "        transforms.RandomRotation(degrees=rotation_degree, fill=0),\n",
        "        transforms.ColorJitter(brightness=brightness, contrast=contrast, saturation=saturation, hue=hue),\n",
        "        transforms.RandomApply([transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))], p=0.3),\n",
        "        transforms.RandomApply([transforms.RandomPerspective(distortion_scale=0.2, p=0.5)], p=0.3),\n",
        "        transforms.Resize((224, 224)),          # <-- This line ENSURES fixed output size!\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                            std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                            std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "\n",
        "    # Dataset loading/splitting\n",
        "    data_dir = \"/content/datasets\"\n",
        "    from torchvision.datasets import ImageFolder\n",
        "    full_dataset = ImageFolder(data_dir)\n",
        "    num_classes = len(full_dataset.classes)\n",
        "\n",
        "    from sklearn.model_selection import StratifiedShuffleSplit\n",
        "    labels = [s[1] for s in full_dataset.samples]\n",
        "    indices = list(range(len(full_dataset)))\n",
        "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "    train_idx, val_idx = next(splitter.split(indices, labels))\n",
        "\n",
        "    train_samples = [full_dataset.samples[i] for i in train_idx]\n",
        "    val_samples = [full_dataset.samples[i] for i in val_idx]\n",
        "\n",
        "    class_to_idx = full_dataset.class_to_idx\n",
        "\n",
        "    # Custom Dataset class to map samples with transform\n",
        "    class CustomDataset(torch.utils.data.Dataset):\n",
        "        def __init__(self, samples, transform, class_to_idx):\n",
        "            self.samples = samples\n",
        "            self.transform = transform\n",
        "            self.class_to_idx = class_to_idx\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.samples)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            path, label = self.samples[idx]\n",
        "            img = Image.open(path).convert('RGB')\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            return img, label\n",
        "\n",
        "    train_dataset = CustomDataset(train_samples, transform=augmentations, class_to_idx=class_to_idx)\n",
        "    val_dataset = CustomDataset(val_samples, transform=val_transform, class_to_idx=class_to_idx)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Build model\n",
        "    backbone = models.efficientnet_b0(pretrained=True)\n",
        "    in_features = backbone.classifier[1].in_features\n",
        "    classifier_head = get_classifier(num_layers, in_features, num_classes, dropout_rate, hidden_units, activation_name)\n",
        "    backbone.classifier = classifier_head\n",
        "\n",
        "    model = backbone.to(device)\n",
        "\n",
        "    # Define criterion\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Define optimizer\n",
        "    if optimizer_name == 'Adam':\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    elif optimizer_name == 'AdamW':\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    else:\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay, momentum=0.9)\n",
        "\n",
        "    # Define scheduler\n",
        "    scheduler = get_scheduler(optimizer, scheduler_name, scheduler_patience)\n",
        "\n",
        "    # Early stopping variables\n",
        "    best_val_acc = 0\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            running_corrects += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_loss = running_loss / total\n",
        "        train_acc = running_corrects / total * 100\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_corrects = 0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                preds = outputs.argmax(dim=1)\n",
        "                val_corrects += (preds == labels).sum().item()\n",
        "                val_total += labels.size(0)\n",
        "\n",
        "        val_loss /= val_total\n",
        "        val_acc = val_corrects / val_total * 100\n",
        "\n",
        "        if scheduler:\n",
        "            if scheduler_name == 'ReduceLROnPlateau':\n",
        "                scheduler.step(val_loss)\n",
        "            else:\n",
        "                scheduler.step()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} \"\n",
        "              f\"Train Loss: {train_loss:.4f} Train Acc: {train_acc:.2f}% \"\n",
        "              f\"Val Loss: {val_loss:.4f} Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # Early stopping check\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= early_stopping_patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "        # Report to Optuna for pruning\n",
        "        trial.report(val_acc, epoch)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return best_val_acc\n",
        "\n",
        "# Running the study\n",
        "if __name__ == \"__main__\":\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "    study.optimize(objective, n_trials=15, timeout=6*60*60)  # 6 hours timeout\n",
        "\n",
        "    print(\"Best hyperparameters found:\")\n",
        "    print(study.best_trial.params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0iz-qQIqnRC",
        "outputId": "a511b390-59d4-4659-ee25-b89ce0b0d59c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-08-02 15:50:54,480] A new study created in memory with name: no-name-d768cb94-5913-443b-934f-65b301fe2adc\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 Train Loss: 1.9591 Train Acc: 47.85% Val Loss: 1.0059 Val Acc: 76.71%\n",
            "Epoch 2/5 Train Loss: 0.7685 Train Acc: 79.77% Val Loss: 0.4790 Val Acc: 86.17%\n",
            "Epoch 3/5 Train Loss: 0.4781 Train Acc: 85.78% Val Loss: 0.3759 Val Acc: 88.26%\n",
            "Epoch 4/5 Train Loss: 0.3804 Train Acc: 88.37% Val Loss: 0.3509 Val Acc: 89.26%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-08-02 17:32:15,394] Trial 0 finished with value: 89.80891719745223 and parameters: {'lr': 7.089036358695856e-05, 'weight_decay': 0.00016006913322157032, 'dropout_rate': 0.12255201954274884, 'batch_size': 32, 'optimizer': 'AdamW', 'num_classifier_layers': 1, 'hidden_units': 128, 'activation': 'relu', 'scheduler': 'CosineAnnealingLR', 'scheduler_patience': 10, 'rotation_degree': 20.50914829759194, 'brightness': 0.29905584790856476, 'contrast': 0.36051251719438027, 'saturation': 0.35152073948456297, 'hue': 0.08182412259570633, 'num_epochs': 5, 'early_stopping_patience': 3}. Best is trial 0 with value: 89.80891719745223.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5 Train Loss: 0.3321 Train Acc: 89.78% Val Loss: 0.3248 Val Acc: 89.81%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2 Train Loss: 1.3810 Train Acc: 56.95% Val Loss: 0.8944 Val Acc: 71.61%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-08-02 18:08:53,703] Trial 1 finished with value: 76.7970882620564 and parameters: {'lr': 0.0006434829794286229, 'weight_decay': 0.0019676077185097856, 'dropout_rate': 0.32979602305510325, 'batch_size': 16, 'optimizer': 'Adam', 'num_classifier_layers': 2, 'hidden_units': 256, 'activation': 'leaky_relu', 'scheduler': 'None', 'scheduler_patience': 5, 'rotation_degree': 11.397607967282626, 'brightness': 0.128944893594884, 'contrast': 0.267874650763847, 'saturation': 0.4314453514425377, 'hue': 0.12949254315477438, 'num_epochs': 2, 'early_stopping_patience': 5}. Best is trial 0 with value: 89.80891719745223.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/2 Train Loss: 0.9672 Train Acc: 71.01% Val Loss: 0.7598 Val Acc: 76.80%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2 Train Loss: 2.0541 Train Acc: 35.40% Val Loss: 1.0593 Val Acc: 72.70%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-08-02 18:45:32,512] Trial 2 finished with value: 86.07825295723384 and parameters: {'lr': 9.4807291742241e-05, 'weight_decay': 0.0001146416505961318, 'dropout_rate': 0.3450917067440256, 'batch_size': 16, 'optimizer': 'Adam', 'num_classifier_layers': 2, 'hidden_units': 128, 'activation': 'leaky_relu', 'scheduler': 'StepLR', 'scheduler_patience': 3, 'rotation_degree': 37.13763183893976, 'brightness': 0.15294835877011093, 'contrast': 0.36740312939820685, 'saturation': 0.3582879572587506, 'hue': 0.02075664158047861, 'num_epochs': 2, 'early_stopping_patience': 4}. Best is trial 0 with value: 89.80891719745223.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/2 Train Loss: 0.9489 Train Acc: 71.95% Val Loss: 0.4821 Val Acc: 86.08%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6 Train Loss: 2.3399 Train Acc: 27.92% Val Loss: 2.0136 Val Acc: 50.77%\n",
            "Epoch 2/6 Train Loss: 1.6456 Train Acc: 55.09% Val Loss: 1.0300 Val Acc: 74.52%\n",
            "Epoch 3/6 Train Loss: 0.9966 Train Acc: 72.56% Val Loss: 0.6283 Val Acc: 81.26%\n",
            "Epoch 4/6 Train Loss: 0.7155 Train Acc: 78.32% Val Loss: 0.5091 Val Acc: 84.35%\n",
            "Epoch 5/6 Train Loss: 0.5910 Train Acc: 81.41% Val Loss: 0.4315 Val Acc: 85.90%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-08-02 20:34:01,827] Trial 3 finished with value: 86.16924476797088 and parameters: {'lr': 0.0009701245298630597, 'weight_decay': 2.34932786859851e-06, 'dropout_rate': 0.326726679674586, 'batch_size': 16, 'optimizer': 'SGD', 'num_classifier_layers': 1, 'hidden_units': 256, 'activation': 'relu', 'scheduler': 'CosineAnnealingLR', 'scheduler_patience': 7, 'rotation_degree': 36.06641185994404, 'brightness': 0.1616361362602513, 'contrast': 0.2992367670321473, 'saturation': 0.15740398361932106, 'hue': 0.11013469125013171, 'num_epochs': 6, 'early_stopping_patience': 5}. Best is trial 0 with value: 89.80891719745223.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/6 Train Loss: 0.5412 Train Acc: 82.55% Val Loss: 0.4186 Val Acc: 86.17%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4 Train Loss: 2.4533 Train Acc: 16.93% Val Loss: 2.3631 Val Acc: 39.40%\n",
            "Epoch 2/4 Train Loss: 2.1676 Train Acc: 32.70% Val Loss: 1.7816 Val Acc: 43.68%\n",
            "Epoch 3/4 Train Loss: 1.7894 Train Acc: 44.44% Val Loss: 1.4456 Val Acc: 57.96%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-08-02 21:46:41,704] Trial 4 finished with value: 66.2420382165605 and parameters: {'lr': 2.31990490669854e-05, 'weight_decay': 7.338254609394071e-06, 'dropout_rate': 0.3362305246587246, 'batch_size': 16, 'optimizer': 'Adam', 'num_classifier_layers': 2, 'hidden_units': 128, 'activation': 'leaky_relu', 'scheduler': 'CosineAnnealingLR', 'scheduler_patience': 8, 'rotation_degree': 25.621364751158055, 'brightness': 0.44324969595344654, 'contrast': 0.46806360105119194, 'saturation': 0.2519100190394554, 'hue': 0.08096694093026992, 'num_epochs': 4, 'early_stopping_patience': 7}. Best is trial 0 with value: 89.80891719745223.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/4 Train Loss: 1.4806 Train Acc: 54.65% Val Loss: 1.1321 Val Acc: 66.24%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "[I 2025-08-02 22:11:55,386] Trial 5 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4 Train Loss: 2.4428 Train Acc: 20.23% Val Loss: 2.1862 Val Acc: 33.21%\n",
            "Best hyperparameters found:\n",
            "{'lr': 7.089036358695856e-05, 'weight_decay': 0.00016006913322157032, 'dropout_rate': 0.12255201954274884, 'batch_size': 32, 'optimizer': 'AdamW', 'num_classifier_layers': 1, 'hidden_units': 128, 'activation': 'relu', 'scheduler': 'CosineAnnealingLR', 'scheduler_patience': 10, 'rotation_degree': 20.50914829759194, 'brightness': 0.29905584790856476, 'contrast': 0.36051251719438027, 'saturation': 0.35152073948456297, 'hue': 0.08182412259570633, 'num_epochs': 5, 'early_stopping_patience': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1"
      ],
      "metadata": {
        "id": "2yxYEPbs0W6B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baseline Model\n",
        "\n",
        "* Architecture: Pretrained ResNet18 (final layer adjusted for number of classes)\n",
        "* Input: Images resized to 224×224, no augmentation\n",
        "* Optimizer: SGD, learning rate 0.01 (default)\n",
        "* Training: 10 epochs, single train/val split (80/20)\n",
        "* Loss Function: CrossEntropyLoss\n",
        "* No tuning: No learning rate schedules, no early stopping, no regularization\n",
        "* Progress visualization: Uses tqdm for per-batch progress bars\n",
        "\n"
      ],
      "metadata": {
        "id": "j7oWFStAtpqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from tqdm import tqdm   # << NEW: tqdm import\n",
        "import os\n",
        "\n",
        "# --------- PARAMETERS -----------\n",
        "data_dir = \"/content/datasets\"\n",
        "image_size = 224\n",
        "batch_size = 32\n",
        "num_epochs = 3\n",
        "learning_rate = 0.01\n",
        "\n",
        "# --------- DATA LOADERS ----------\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "full_dataset = datasets.ImageFolder(data_dir, transform=transform)\n",
        "num_classes = len(full_dataset.classes)\n",
        "\n",
        "val_size = int(0.2 * len(full_dataset))\n",
        "train_size = len(full_dataset) - val_size\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# --------- MODEL ----------\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# --------- OPTIMIZER, LOSS -------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# --------- TRAIN LOOP WITH TQDM -----------\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Wrap the training loader with tqdm!\n",
        "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    train_acc = 100.0 * correct / total\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {avg_loss:.4f} Train Acc: {train_acc:.2f}%\")\n",
        "\n",
        "    # ------ Validate ------\n",
        "    model.eval()\n",
        "    val_correct, val_total = 0, 0\n",
        "    val_loss = 0.0\n",
        "    # tqdm for validation loader as well\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "    val_acc = 100.0 * val_correct / val_total\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f\"         Val Loss: {avg_val_loss:.4f} Val Acc: {val_acc:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0I9oNBgpS59",
        "outputId": "9ea32a0f-c39f-47d4-ac64-599aa72071f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 81.0MB/s]\n",
            "Epoch 1/3 - Training: 100%|██████████| 138/138 [18:35<00:00,  8.08s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/3] Train Loss: 1.0579 Train Acc: 71.72%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Validation: 100%|██████████| 35/35 [01:33<00:00,  2.67s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Val Loss: 0.5064 Val Acc: 85.79%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training: 100%|██████████| 138/138 [18:50<00:00,  8.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/3] Train Loss: 0.4098 Train Acc: 88.79%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Validation: 100%|██████████| 35/35 [01:32<00:00,  2.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Val Loss: 0.4027 Val Acc: 87.98%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training: 100%|██████████| 138/138 [18:34<00:00,  8.08s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/3] Train Loss: 0.2576 Train Acc: 93.40%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Validation: 100%|██████████| 35/35 [01:34<00:00,  2.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Val Loss: 0.3746 Val Acc: 87.98%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2"
      ],
      "metadata": {
        "id": "IfaKQPis0NqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic Data Augmentation\n",
        "\n",
        "* Architecture: Same as Step 1 (ResNet18, last layer adjusted).\n",
        "* Data Augmentation:\n",
        "* * Training set: Includes\n",
        "* * RandomHorizontalFlip and RandomCrop to increase dataset variability.\n",
        "Validation set: Only resizing (no augmentation).\n",
        "* Other settings:\n",
        "* * SGD optimizer, learning rate 0.01, 10 epochs, 80/20 split.\n",
        "* * No scheduler, no early stopping, no model saving.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IRKxZpSJt6gK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# --------- PARAMETERS -----------\n",
        "data_dir = \"/content/datasets\"\n",
        "image_size = 224\n",
        "batch_size = 32\n",
        "num_epochs = 3\n",
        "learning_rate = 0.01\n",
        "\n",
        "# --------- DATA LOADERS WITH AUGMENTATION ----------\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomCrop(image_size),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Build one ImageFolder FOR CLASS MAPPING\n",
        "full_dataset = datasets.ImageFolder(data_dir)\n",
        "num_classes = len(full_dataset.classes)\n",
        "val_size = int(0.2 * len(full_dataset))\n",
        "train_size = len(full_dataset) - val_size\n",
        "\n",
        "# Split indices for train/val\n",
        "indices = torch.randperm(len(full_dataset))\n",
        "train_indices = indices[:train_size]\n",
        "val_indices = indices[train_size:]\n",
        "\n",
        "# Clone datasets and assign proper transforms and class mapping\n",
        "train_dataset = datasets.ImageFolder(data_dir, transform=transform_train)\n",
        "val_dataset = datasets.ImageFolder(data_dir, transform=transform_val)\n",
        "# Share class mapping (CRITICAL)\n",
        "train_dataset.classes = full_dataset.classes\n",
        "train_dataset.class_to_idx = full_dataset.class_to_idx\n",
        "val_dataset.classes = full_dataset.classes\n",
        "val_dataset.class_to_idx = full_dataset.class_to_idx\n",
        "\n",
        "# Use Subset to partition\n",
        "train_dataset = torch.utils.data.Subset(train_dataset, train_indices)\n",
        "val_dataset = torch.utils.data.Subset(val_dataset, val_indices)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# --------- MODEL ----------\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# --------- OPTIMIZER, LOSS -------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# --------- TRAIN LOOP WITH TQDM -----------\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    train_acc = 100.0 * correct / total\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {avg_loss:.4f} Train Acc: {train_acc:.2f}%\")\n",
        "\n",
        "    model.eval()\n",
        "    val_correct, val_total = 0, 0\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "    val_acc = 100.0 * val_correct / val_total\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f\"         Val Loss: {avg_val_loss:.4f} Val Acc: {val_acc:.2f}%\")\n"
      ],
      "metadata": {
        "id": "SJ5X_rvW0Ld5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "outputId": "4916f5a9-9c85-4cff-b66f-b90ce9f96f2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Epoch 1/3 - Training: 100%|██████████| 138/138 [17:48<00:00,  7.75s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/3] Train Loss: 1.0388 Train Acc: 72.52%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Validation: 100%|██████████| 35/35 [01:28<00:00,  2.52s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Val Loss: 0.5226 Val Acc: 84.79%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training:  98%|█████████▊| 135/138 [17:45<00:23,  7.89s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-275960807.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3"
      ],
      "metadata": {
        "id": "l7ih_Stp13MV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizer & Learning Rate Tuning\n",
        "\n",
        "* Architecture: Still using ResNet18.\n",
        "* Data Augmentation: Same as Step 2 (random crop, horizontal flip).\n",
        "* Optimizer: Switched from SGD to Adam for usually better/faster convergence.\n",
        "* Learning Rate: Typically start with a lower learning rate for Adam (e.g., 0.001 or 0.0001).\n",
        "* Optional: Add a simple scheduler like StepLR for basic learning rate decay.\n",
        "* Other: 10 epochs, same train/val split, no other tuning yet.\n"
      ],
      "metadata": {
        "id": "09shQbaQuLAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# --------- PARAMETERS -----------\n",
        "data_dir = \"/content/datasets\"\n",
        "image_size = 224\n",
        "batch_size = 32\n",
        "num_epochs = 3\n",
        "learning_rate = 1e-4     # Lowered for Adam\n",
        "\n",
        "# --------- DATA LOADERS WITH AUGMENTATION ----------\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomCrop(image_size),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "full_dataset = datasets.ImageFolder(data_dir, transform=transform_train)\n",
        "num_classes = len(full_dataset.classes)\n",
        "val_size = int(0.2 * len(full_dataset))\n",
        "train_size = len(full_dataset) - val_size\n",
        "\n",
        "# Assign transforms for train/val split\n",
        "train_indices = torch.arange(train_size)\n",
        "val_indices = torch.arange(train_size, train_size + val_size)\n",
        "train_dataset = torch.utils.data.Subset(full_dataset, train_indices)\n",
        "val_dataset_raw = datasets.ImageFolder(data_dir, transform=transform_val)\n",
        "val_dataset = torch.utils.data.Subset(val_dataset_raw, val_indices)\n",
        "\n",
        "# Share class mapping (CRITICAL)\n",
        "train_dataset.classes = full_dataset.classes\n",
        "train_dataset.class_to_idx = full_dataset.class_to_idx\n",
        "val_dataset.classes = full_dataset.classes\n",
        "val_dataset.class_to_idx = full_dataset.class_to_i\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# --------- MODEL ----------\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# --------- OPTIMIZER, SCHEDULER, LOSS -------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "\n",
        "# --------- TRAIN LOOP WITH TQDM -----------\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    train_acc = 100.0 * correct / total\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {avg_loss:.4f} Train Acc: {train_acc:.2f}%\")\n",
        "\n",
        "    model.eval()\n",
        "    val_correct, val_total = 0, 0\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "    val_acc = 100.0 * val_correct / val_total\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f\"         Val Loss: {avg_val_loss:.4f} Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    scheduler.step()  # ADVANCES THE LR SCHEDULE\n",
        "\n"
      ],
      "metadata": {
        "id": "5R-gsDOh15RB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc44effc-824d-4701-c2e0-797e8d70ef89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training: 100%|██████████| 138/138 [18:57<00:00,  8.25s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/3] Train Loss: 0.7209 Train Acc: 78.94%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Validation: 100%|██████████| 35/35 [01:31<00:00,  2.60s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Val Loss: 5.6382 Val Acc: 10.29%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training: 100%|██████████| 138/138 [18:38<00:00,  8.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/3] Train Loss: 0.2724 Train Acc: 91.79%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Validation: 100%|██████████| 35/35 [01:31<00:00,  2.63s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Val Loss: 6.1990 Val Acc: 10.20%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training: 100%|██████████| 138/138 [18:31<00:00,  8.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/3] Train Loss: 0.1573 Train Acc: 95.43%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Validation: 100%|██████████| 35/35 [01:31<00:00,  2.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Val Loss: 7.0780 Val Acc: 10.38%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4"
      ],
      "metadata": {
        "id": "1lQXWUuv153P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Increase Model Complexity\n",
        "* Instead of ResNet18, use a deeper/bigger model like ResNet50 or EfficientNet-B0.\n",
        "*  Keep the previous data augmentation from Step 3.\n",
        "* Keep optimizer and learning rate tuning unchanged (Adam with lr=1e-4).\n",
        "* Optionally, add a little dropout in the classifier head to help regularization, but this is optional here (will be covered more in Step 5).\n"
      ],
      "metadata": {
        "id": "fg6JbhCBuVeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "data_dir = \"/content/datasets\"\n",
        "image_size = 224\n",
        "batch_size = 32\n",
        "num_epochs = 3\n",
        "learning_rate = 1e-4\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomCrop(image_size),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "full_dataset = datasets.ImageFolder(data_dir, transform=transform_train)\n",
        "num_classes = len(full_dataset.classes)\n",
        "\n",
        "val_size = int(0.2 * len(full_dataset))\n",
        "train_size = len(full_dataset) - val_size\n",
        "\n",
        "train_dataset = torch.utils.data.Subset(full_dataset, range(train_size))\n",
        "val_dataset_raw = datasets.ImageFolder(data_dir, transform=transform_val)\n",
        "val_dataset = torch.utils.data.Subset(val_dataset_raw, range(train_size, train_size + val_size))\n",
        "\n",
        "# Share class mapping (CRITICAL)\n",
        "train_dataset.classes = full_dataset.classes\n",
        "train_dataset.class_to_idx = full_dataset.class_to_idx\n",
        "val_dataset.classes = full_dataset.classes\n",
        "val_dataset.class_to_idx = full_dataset.class_to_i\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# -- Model change: EfficientNet-B0 or ResNet50 --\n",
        "model = models.efficientnet_b0(pretrained=True)\n",
        "model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
        "# Alternative:\n",
        "# model = models.resnet50(pretrained=True)\n",
        "# model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    train_acc = 100.0 * correct / total\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {avg_loss:.4f} Train Acc: {train_acc:.2f}%\")\n",
        "\n",
        "    model.eval()\n",
        "    val_correct, val_total = 0, 0\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "    val_acc = 100.0 * val_correct / val_total\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f\"         Val Loss: {avg_val_loss:.4f} Val Acc: {val_acc:.2f}%\")\n"
      ],
      "metadata": {
        "id": "M3FrXVxU17w6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec73e213-585d-4d1c-f585-f1d398c99085"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
            "100%|██████████| 20.5M/20.5M [00:00<00:00, 153MB/s]\n",
            "Epoch 1/3 - Training: 100%|██████████| 138/138 [17:26<00:00,  7.58s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/3] Train Loss: 1.3315 Train Acc: 65.42%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Validation: 100%|██████████| 35/35 [00:58<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Val Loss: 4.9680 Val Acc: 10.29%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training: 100%|██████████| 138/138 [16:55<00:00,  7.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/3] Train Loss: 0.4008 Train Acc: 88.74%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Validation: 100%|██████████| 35/35 [00:52<00:00,  1.51s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Val Loss: 5.9329 Val Acc: 10.29%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training: 100%|██████████| 138/138 [17:04<00:00,  7.43s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/3] Train Loss: 0.2506 Train Acc: 92.33%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Validation: 100%|██████████| 35/35 [00:55<00:00,  1.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Val Loss: 6.8139 Val Acc: 10.29%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5"
      ],
      "metadata": {
        "id": "TgrS8Pl_2eSD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Better Augmentation & Regularization\n",
        "\n",
        "* Advanced Data Augmentation: Add stronger and more varied image augmentations (e.g. ColorJitter, RandomRotation).\n",
        "* Regularization: Use weight decay (L2 regularization) in the optimizer and add dropout in the classifier head if your model supports it.\n",
        "* Architecture: Same as Step 4 (EfficientNet-B0 or ResNet50).\n"
      ],
      "metadata": {
        "id": "vfcof-2guhe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "data_dir = \"/content/datasets\"\n",
        "image_size = 224\n",
        "batch_size = 32\n",
        "num_epochs = 3\n",
        "learning_rate = 1e-4\n",
        "weight_decay = 1e-4\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomCrop(image_size),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "full_dataset = datasets.ImageFolder(data_dir, transform=transform_train)\n",
        "num_classes = len(full_dataset.classes)\n",
        "\n",
        "val_size = int(0.2 * len(full_dataset))\n",
        "train_size = len(full_dataset) - val_size\n",
        "\n",
        "train_dataset = torch.utils.data.Subset(full_dataset, range(train_size))\n",
        "val_dataset_raw = datasets.ImageFolder(data_dir, transform=transform_val)\n",
        "val_dataset = torch.utils.data.Subset(val_dataset_raw, range(train_size, train_size + val_size))\n",
        "\n",
        "# Share class mapping (CRITICAL)\n",
        "train_dataset.classes = full_dataset.classes\n",
        "train_dataset.class_to_idx = full_dataset.class_to_idx\n",
        "val_dataset.classes = full_dataset.classes\n",
        "val_dataset.class_to_idx = full_dataset.class_to_i\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Model with dropout in classifier\n",
        "model = models.efficientnet_b0(pretrained=True)\n",
        "in_features = model.classifier[1].in_features\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(in_features, num_classes)\n",
        ")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    train_acc = 100.0 * correct / total\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {avg_loss:.4f} Train Acc: {train_acc:.2f}%\")\n",
        "\n",
        "    model.eval()\n",
        "    val_correct, val_total = 0, 0\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "    val_acc = 100.0 * val_correct / val_total\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f\"         Val Loss: {avg_val_loss:.4f} Val Acc: {val_acc:.2f}%\")\n"
      ],
      "metadata": {
        "id": "XrlxqM4_2gaT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5e21f3e-8e9e-4a62-f6bf-d056e74849bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training: 100%|██████████| 138/138 [17:40<00:00,  7.68s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/3] Train Loss: 1.4235 Train Acc: 61.60%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Validation: 100%|██████████| 35/35 [00:56<00:00,  1.62s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Val Loss: 4.9919 Val Acc: 10.29%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training: 100%|██████████| 138/138 [17:50<00:00,  7.75s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/3] Train Loss: 0.4827 Train Acc: 86.31%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Validation: 100%|██████████| 35/35 [00:57<00:00,  1.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Val Loss: 5.7971 Val Acc: 10.38%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training: 100%|██████████| 138/138 [17:46<00:00,  7.73s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/3] Train Loss: 0.3065 Train Acc: 90.65%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Validation: 100%|██████████| 35/35 [00:54<00:00,  1.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Val Loss: 6.6921 Val Acc: 10.38%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6"
      ],
      "metadata": {
        "id": "lNtJPCRs4A7X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adaptive Learning Rate, Early Stopping & Model Saving\n",
        "\n",
        "* Adaptive Learning Rate: Uses ReduceLROnPlateau, which automatically lowers the learning rate if the validation loss plateaus.\n",
        "* Early Stopping: Stops training if validation accuracy hasn’t improved for a set number of epochs (patience), preventing overfitting.\n",
        "* Model Saving: Stores the weights of the best model (with highest validation accuracy).\n",
        "* Regularization & Augmentation: Retains advanced transformations, dropout, and weight decay from Step 5.\n"
      ],
      "metadata": {
        "id": "i7VNVI3wup3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "data_dir = \"/content/datasets\"\n",
        "image_size = 224\n",
        "batch_size = 32\n",
        "num_epochs = 50             # More epochs (we now stop early if no improvement)\n",
        "learning_rate = 1e-4\n",
        "weight_decay = 1e-4\n",
        "patience = 10               # Early stopping patience\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomCrop(image_size),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "full_dataset = datasets.ImageFolder(data_dir, transform=transform_train)\n",
        "num_classes = len(full_dataset.classes)\n",
        "\n",
        "val_size = int(0.2 * len(full_dataset))\n",
        "train_size = len(full_dataset) - val_size\n",
        "\n",
        "train_dataset = torch.utils.data.Subset(full_dataset, range(train_size))\n",
        "val_dataset_raw = datasets.ImageFolder(data_dir, transform=transform_val)\n",
        "val_dataset = torch.utils.data.Subset(val_dataset_raw, range(train_size, train_size + val_size))\n",
        "\n",
        "# Share class mapping (CRITICAL)\n",
        "train_dataset.classes = full_dataset.classes\n",
        "train_dataset.class_to_idx = full_dataset.class_to_idx\n",
        "val_dataset.classes = full_dataset.classes\n",
        "val_dataset.class_to_idx = full_dataset.class_to_i\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "model = models.efficientnet_b0(pretrained=True)\n",
        "in_features = model.classifier[1].in_features\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(in_features, num_classes)\n",
        ")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
        "\n",
        "best_val_acc = 0.0\n",
        "epochs_no_improve = 0\n",
        "best_model_path = \"best_model.pth\"\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    train_acc = 100.0 * correct / total\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {avg_loss:.4f} Train Acc: {train_acc:.2f}%\")\n",
        "\n",
        "    model.eval()\n",
        "    val_correct, val_total = 0, 0\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "    val_acc = 100.0 * val_correct / val_total\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f\"         Val Loss: {avg_val_loss:.4f} Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    scheduler.step(avg_val_loss)  # Adjust learning rate based on val loss\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        epochs_no_improve = 0\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(\"New best model saved!\")\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        if epochs_no_improve > patience:\n",
        "            print(\"Early stopping triggered!\")\n",
        "            break\n"
      ],
      "metadata": {
        "id": "ly4loRja4CVD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43b2bbd0-69dd-433a-a480-f23aaf905174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Epoch 1/50 - Training: 100%|██████████| 138/138 [17:17<00:00,  7.52s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50] Train Loss: 1.4658 Train Acc: 60.83%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/50 - Validation: 100%|██████████| 35/35 [00:54<00:00,  1.55s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Val Loss: 4.7976 Val Acc: 10.38%\n",
            "New best model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/50 - Training: 100%|██████████| 138/138 [17:31<00:00,  7.62s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/50] Train Loss: 0.4928 Train Acc: 85.85%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/50 - Validation: 100%|██████████| 35/35 [00:54<00:00,  1.57s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Val Loss: 6.0092 Val Acc: 10.38%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/50 - Training: 100%|██████████| 138/138 [17:28<00:00,  7.60s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/50] Train Loss: 0.3211 Train Acc: 90.40%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/50 - Validation: 100%|██████████| 35/35 [00:56<00:00,  1.61s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Val Loss: 6.6526 Val Acc: 10.38%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/50 - Training: 100%|██████████| 138/138 [17:41<00:00,  7.69s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/50] Train Loss: 0.2385 Train Acc: 92.68%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/50 - Validation: 100%|██████████| 35/35 [00:53<00:00,  1.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Val Loss: 7.3696 Val Acc: 10.38%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/50 - Training: 100%|██████████| 138/138 [17:37<00:00,  7.67s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/50] Train Loss: 0.1826 Train Acc: 94.54%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/50 - Validation: 100%|██████████| 35/35 [00:53<00:00,  1.53s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Val Loss: 7.6812 Val Acc: 10.38%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/50 - Training: 100%|██████████| 138/138 [17:43<00:00,  7.71s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/50] Train Loss: 0.1405 Train Acc: 95.38%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/50 - Validation: 100%|██████████| 35/35 [00:55<00:00,  1.58s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Val Loss: 7.3781 Val Acc: 10.38%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/50 - Training: 100%|██████████| 138/138 [17:42<00:00,  7.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/50] Train Loss: 0.1214 Train Acc: 96.25%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/50 - Validation: 100%|██████████| 35/35 [00:54<00:00,  1.57s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Val Loss: 7.7385 Val Acc: 10.38%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/50 - Training: 100%|██████████| 138/138 [17:34<00:00,  7.64s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/50] Train Loss: 0.0878 Train Acc: 97.43%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/50 - Validation: 100%|██████████| 35/35 [00:53<00:00,  1.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Val Loss: 7.9762 Val Acc: 10.38%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/50 - Training: 100%|██████████| 138/138 [17:35<00:00,  7.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/50] Train Loss: 0.0737 Train Acc: 97.82%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/50 - Validation: 100%|██████████| 35/35 [00:55<00:00,  1.59s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Val Loss: 8.2179 Val Acc: 10.38%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/50 - Training: 100%|██████████| 138/138 [17:35<00:00,  7.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/50] Train Loss: 0.0664 Train Acc: 98.25%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/50 - Validation: 100%|██████████| 35/35 [00:55<00:00,  1.57s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Val Loss: 8.6251 Val Acc: 10.38%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/50 - Training:  56%|█████▌    | 77/138 [09:56<07:34,  7.45s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7"
      ],
      "metadata": {
        "id": "YoKER0Gn434f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Improved Pest Classification Training Pipeline\n",
        "===========================================\n",
        "\n",
        "Modern training pipeline using EfficientNet-B0 with comprehensive validation,\n",
        "agricultural-specific augmentations, and uncertainty quantification.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional, Union\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Try to import ML dependencies\n",
        "try:\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.optim as optim\n",
        "    import torch.nn.functional as F\n",
        "    from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "    from torchvision import transforms, models\n",
        "    from torchvision.datasets import ImageFolder\n",
        "    import torchvision.transforms.functional as TF\n",
        "    from PIL import Image\n",
        "    ML_AVAILABLE = True\n",
        "    logger.info(f\"Full ML dependencies loaded successfully - PyTorch {torch.__version__}\")\n",
        "\n",
        "    # Check PyTorch version for compatibility\n",
        "    torch_version = tuple(map(int, torch.__version__.split('.')[:2]))\n",
        "    if torch_version < (1, 8):\n",
        "        logger.warning(f\"PyTorch version {torch.__version__} is quite old. Consider upgrading for best compatibility.\")\n",
        "\n",
        "except ImportError as e:\n",
        "    ML_AVAILABLE = False\n",
        "    logger.error(f\"ML dependencies not available: {e}\")\n",
        "    print(\"Please install: pip install torch torchvision matplotlib seaborn scikit-learn\")\n",
        "    sys.exit(1)\n",
        "\n",
        "\n",
        "class AgriculturalAugmentations:\n",
        "    \"\"\"Agricultural-specific data augmentations for pest images.\"\"\"\n",
        "\n",
        "    def __init__(self, image_size: int = 224):\n",
        "        self.image_size = image_size\n",
        "\n",
        "    def get_train_transforms(self):\n",
        "        \"\"\"Get training augmentations optimized for pest imagery.\"\"\"\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize((self.image_size + 32, self.image_size + 32)),\n",
        "            transforms.RandomCrop(self.image_size),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomVerticalFlip(p=0.3),  # Pests can be in any orientation\n",
        "            transforms.RandomRotation(degrees=30, fill=0),\n",
        "            transforms.ColorJitter(\n",
        "                brightness=0.3,    # Agricultural lighting varies\n",
        "                contrast=0.3,      # Different background contrasts\n",
        "                saturation=0.2,    # Natural color variations\n",
        "                hue=0.1           # Slight hue shifts\n",
        "            ),\n",
        "            transforms.RandomApply([\n",
        "                transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))\n",
        "            ], p=0.3),\n",
        "            transforms.RandomApply([\n",
        "                transforms.RandomPerspective(distortion_scale=0.2, p=0.5)\n",
        "            ], p=0.3),\n",
        "            # Convert to tensor and normalize\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.485, 0.456, 0.406],  # ImageNet pre-trained mean\n",
        "                std=[0.229, 0.224, 0.225]   # ImageNet pre-trained std\n",
        "            ),\n",
        "            # Additional agricultural-specific augmentations\n",
        "            transforms.RandomApply([\n",
        "                self._add_dirt_spots\n",
        "            ], p=0.2),\n",
        "        ])\n",
        "\n",
        "    def get_val_transforms(self):\n",
        "        \"\"\"Get validation transforms - minimal processing.\"\"\"\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize((self.image_size, self.image_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.485, 0.456, 0.406],\n",
        "                std=[0.229, 0.224, 0.225]\n",
        "            )\n",
        "        ])\n",
        "\n",
        "    def _add_dirt_spots(self, tensor):\n",
        "        \"\"\"Add random dirt spots to simulate real agricultural conditions.\"\"\"\n",
        "        if random.random() < 0.5:\n",
        "            # Add small dark spots\n",
        "            num_spots = random.randint(1, 3)\n",
        "            for _ in range(num_spots):\n",
        "                x = random.randint(0, tensor.shape[1] - 5)\n",
        "                y = random.randint(0, tensor.shape[2] - 5)\n",
        "                spot_size = random.randint(2, 4)\n",
        "                tensor[:, x:x+spot_size, y:y+spot_size] *= random.uniform(0.3, 0.7)\n",
        "        return tensor\n",
        "\n",
        "\n",
        "class ImprovedPestDataset(Dataset):\n",
        "    \"\"\"Enhanced dataset class with better handling of pest images.\"\"\"\n",
        "\n",
        "    def __init__(self, data_dir: str, transform=None, class_mapping: Dict = None):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "        self.classes = []\n",
        "        self.class_to_idx = {}\n",
        "\n",
        "        # Build dataset\n",
        "        self._build_dataset(class_mapping)\n",
        "\n",
        "    def _build_dataset(self, class_mapping: Dict = None):\n",
        "        \"\"\"Build dataset with proper class mapping.\"\"\"\n",
        "        # Get all pest directories\n",
        "        pest_dirs = [d for d in self.data_dir.iterdir() if d.is_dir()]\n",
        "\n",
        "        if class_mapping and isinstance(class_mapping, dict):\n",
        "            # Check if it's a proper class mapping with 'classes' and 'class_to_idx'\n",
        "            if 'classes' in class_mapping and 'class_to_idx' in class_mapping:\n",
        "                self.classes = class_mapping['classes']\n",
        "                self.class_to_idx = class_mapping['class_to_idx']\n",
        "            elif isinstance(class_mapping, dict) and all(isinstance(v, int) for v in class_mapping.values()):\n",
        "                # Direct mapping format {class_name: index}\n",
        "                self.classes = list(class_mapping.keys())\n",
        "                self.class_to_idx = class_mapping\n",
        "            else:\n",
        "                # Fallback to auto-detect\n",
        "                self.classes = sorted([d.name for d in pest_dirs])\n",
        "                self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
        "        else:\n",
        "            # Auto-detect classes\n",
        "            self.classes = sorted([d.name for d in pest_dirs])\n",
        "            self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
        "\n",
        "        # Build samples list\n",
        "        for pest_dir in pest_dirs:\n",
        "            if pest_dir.name not in self.class_to_idx:\n",
        "                continue\n",
        "\n",
        "            class_idx = self.class_to_idx[pest_dir.name]\n",
        "\n",
        "            # Get all image files\n",
        "            image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']\n",
        "            image_files = []\n",
        "            for ext in image_extensions:\n",
        "                image_files.extend(list(pest_dir.glob(ext)))\n",
        "\n",
        "            # Add to samples\n",
        "            for img_path in image_files:\n",
        "                self.samples.append((str(img_path), class_idx))\n",
        "\n",
        "        logger.info(f\"Dataset built: {len(self.samples)} samples, {len(self.classes)} classes\")\n",
        "        logger.info(f\"Classes: {self.classes}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, class_idx = self.samples[idx]\n",
        "\n",
        "        # Load image\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to load image {img_path}: {e}\")\n",
        "            # Return a black image as fallback\n",
        "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
        "\n",
        "        # Apply transforms\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, class_idx\n",
        "\n",
        "    def get_class_distribution(self):\n",
        "        \"\"\"Get distribution of classes in dataset.\"\"\"\n",
        "        class_counts = defaultdict(int)\n",
        "        for _, class_idx in self.samples:\n",
        "            class_counts[class_idx] += 1\n",
        "\n",
        "        return dict(class_counts)\n",
        "\n",
        "\n",
        "class EfficientNetPestClassifier(nn.Module):\n",
        "    \"\"\"EfficientNet-B0 based pest classifier with uncertainty estimation.\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes: int, dropout_rate: float = 0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load pre-trained EfficientNet-B0 (compatible with different PyTorch versions)\n",
        "        try:\n",
        "            # New style (PyTorch 0.13+)\n",
        "            self.backbone = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
        "        except (AttributeError, TypeError):\n",
        "            # Old style (PyTorch < 0.13)\n",
        "            self.backbone = models.efficientnet_b0(pretrained=True)\n",
        "\n",
        "        # Replace classifier head\n",
        "        in_features = self.backbone.classifier[1].in_features\n",
        "        self.backbone.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(in_features, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate / 2),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "        # For uncertainty estimation - add temperature scaling\n",
        "        self.temperature = nn.Parameter(torch.ones(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.backbone(x)\n",
        "        return logits\n",
        "\n",
        "    def forward_with_temperature(self, x):\n",
        "        \"\"\"Forward pass with temperature scaling for calibration.\"\"\"\n",
        "        logits = self.backbone(x)\n",
        "        return logits / self.temperature\n",
        "\n",
        "\n",
        "class ImprovedTrainer:\n",
        "    \"\"\"Modern training pipeline with comprehensive validation.\"\"\"\n",
        "\n",
        "    def __init__(self, data_dir: str, output_dir: str = \"models/improved\"):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Training configuration\n",
        "        self.config = {\n",
        "            'image_size': 224,\n",
        "            'batch_size': 32,\n",
        "            'num_epochs': 100,\n",
        "            'learning_rate': 1e-4,\n",
        "            'weight_decay': 1e-4,\n",
        "            'patience': 15,\n",
        "            'min_delta': 1e-4,\n",
        "            'num_folds': 5,\n",
        "            'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        }\n",
        "\n",
        "        # Initialize components\n",
        "        self.augmentations = AgriculturalAugmentations(self.config['image_size'])\n",
        "        self.device = torch.device(self.config['device'])\n",
        "\n",
        "        logger.info(f\"Training on device: {self.device}\")\n",
        "\n",
        "    def prepare_data(self):\n",
        "        \"\"\"Prepare dataset with stratified splits.\"\"\"\n",
        "        logger.info(\"Preparing dataset...\")\n",
        "\n",
        "        # Load full dataset\n",
        "        dataset = ImprovedPestDataset(\n",
        "            self.data_dir,\n",
        "            transform=self.augmentations.get_val_transforms()  # No augmentation for splitting\n",
        "        )\n",
        "\n",
        "        if len(dataset) == 0:\n",
        "            raise ValueError(f\"No images found in {self.data_dir}\")\n",
        "\n",
        "        # Get class distribution\n",
        "        class_dist = dataset.get_class_distribution()\n",
        "        logger.info(f\"Class distribution: {class_dist}\")\n",
        "\n",
        "        # Check for class imbalance\n",
        "        min_samples = min(class_dist.values())\n",
        "        max_samples = max(class_dist.values())\n",
        "        imbalance_ratio = max_samples / min_samples\n",
        "\n",
        "        if imbalance_ratio > 10:\n",
        "            logger.warning(f\"Severe class imbalance detected (ratio: {imbalance_ratio:.1f})\")\n",
        "\n",
        "        # Store dataset info\n",
        "        self.num_classes = len(dataset.classes)\n",
        "        self.class_names = dataset.classes\n",
        "        self.class_to_idx = dataset.class_to_idx\n",
        "\n",
        "        # Save class mapping\n",
        "        class_mapping = {\n",
        "            'classes': self.class_names,\n",
        "            'class_to_idx': self.class_to_idx,\n",
        "            'num_classes': self.num_classes\n",
        "        }\n",
        "\n",
        "        with open(self.output_dir / 'class_mapping.json', 'w') as f:\n",
        "            json.dump(class_mapping, f, indent=2)\n",
        "\n",
        "        return dataset, class_dist\n",
        "\n",
        "    def create_weighted_sampler(self, dataset, class_dist):\n",
        "        \"\"\"Create weighted sampler to handle class imbalance.\"\"\"\n",
        "        # Calculate weights for each class (inverse frequency)\n",
        "        total_samples = len(dataset)\n",
        "        class_weights = {}\n",
        "\n",
        "        for class_idx, count in class_dist.items():\n",
        "            class_weights[class_idx] = total_samples / (len(class_dist) * count)\n",
        "\n",
        "        # Create sample weights\n",
        "        sample_weights = []\n",
        "        for _, class_idx in dataset.samples:\n",
        "            sample_weights.append(class_weights[class_idx])\n",
        "\n",
        "        return WeightedRandomSampler(\n",
        "            weights=sample_weights,\n",
        "            num_samples=len(sample_weights),\n",
        "            replacement=True\n",
        "        )\n",
        "\n",
        "    def train_fold(self, train_dataset, val_dataset, fold_num: int):\n",
        "        \"\"\"Train a single fold.\"\"\"\n",
        "        logger.info(f\"Training fold {fold_num + 1}/{self.config['num_folds']}\")\n",
        "\n",
        "        # Create data loaders (reduced workers for stability on CPU)\n",
        "        num_workers = 2 if self.device.type == 'cuda' else 0  # 0 workers on CPU to avoid issues\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=self.config['batch_size'],\n",
        "            shuffle=True,\n",
        "            num_workers=num_workers,\n",
        "            pin_memory=True if self.device.type == 'cuda' else False\n",
        "        )\n",
        "\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=self.config['batch_size'],\n",
        "            shuffle=False,\n",
        "            num_workers=num_workers,\n",
        "            pin_memory=True if self.device.type == 'cuda' else False\n",
        "        )\n",
        "\n",
        "        # Initialize model\n",
        "        model = EfficientNetPestClassifier(\n",
        "            num_classes=self.num_classes,\n",
        "            dropout_rate=0.3\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Loss and optimizer\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=self.config['learning_rate'],\n",
        "            weight_decay=self.config['weight_decay']\n",
        "        )\n",
        "\n",
        "        # Learning rate scheduler (compatible with different PyTorch versions)\n",
        "        try:\n",
        "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                optimizer,\n",
        "                mode='min',\n",
        "                factor=0.5,\n",
        "                patience=5,\n",
        "                verbose=True\n",
        "            )\n",
        "        except TypeError:\n",
        "            # Older PyTorch versions don't support verbose parameter\n",
        "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                optimizer,\n",
        "                mode='min',\n",
        "                factor=0.5,\n",
        "                patience=5\n",
        "            )\n",
        "\n",
        "        # Training loop\n",
        "        best_val_acc = 0.0\n",
        "        patience_counter = 0\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        val_accuracies = []\n",
        "\n",
        "        for epoch in range(self.config['num_epochs']):\n",
        "            # Training phase\n",
        "            model.train()\n",
        "            running_loss = 0.0\n",
        "            correct_predictions = 0\n",
        "            total_predictions = 0\n",
        "\n",
        "            for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "                images, labels = images.to(self.device), labels.to(self.device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total_predictions += labels.size(0)\n",
        "                correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "                if batch_idx % 50 == 0:\n",
        "                    logger.info(f'Fold {fold_num+1}, Epoch {epoch+1}, Batch {batch_idx}, '\n",
        "                              f'Loss: {loss.item():.4f}')\n",
        "\n",
        "            # Validation phase\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for images, labels in val_loader:\n",
        "                    images, labels = images.to(self.device), labels.to(self.device)\n",
        "                    outputs = model(images)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    val_loss += loss.item()\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    val_total += labels.size(0)\n",
        "                    val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Calculate metrics\n",
        "            train_acc = 100.0 * correct_predictions / total_predictions\n",
        "            val_acc = 100.0 * val_correct / val_total\n",
        "            avg_train_loss = running_loss / len(train_loader)\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "            train_losses.append(avg_train_loss)\n",
        "            val_losses.append(avg_val_loss)\n",
        "            val_accuracies.append(val_acc)\n",
        "\n",
        "            logger.info(f'Fold {fold_num+1}, Epoch {epoch+1}: '\n",
        "                       f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%, '\n",
        "                       f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "            # Learning rate scheduling\n",
        "            scheduler.step(avg_val_loss)\n",
        "\n",
        "            # Early stopping check\n",
        "            if val_acc > best_val_acc + self.config['min_delta']:\n",
        "                best_val_acc = val_acc\n",
        "                patience_counter = 0\n",
        "\n",
        "                # Save best model\n",
        "                torch.save({\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'epoch': epoch,\n",
        "                    'val_acc': val_acc,\n",
        "                    'fold': fold_num,\n",
        "                    'class_mapping': {\n",
        "                        'classes': self.class_names,\n",
        "                        'class_to_idx': self.class_to_idx,\n",
        "                        'num_classes': self.num_classes\n",
        "                    }\n",
        "                }, self.output_dir / f'best_model_fold_{fold_num}.pth')\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            if patience_counter > self.config['patience']:\n",
        "                logger.info(f'Early stopping at epoch {epoch+1}')\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            'best_val_acc': best_val_acc,\n",
        "            'train_losses': train_losses,\n",
        "            'val_losses': val_losses,\n",
        "            'val_accuracies': val_accuracies,\n",
        "            'final_model': model\n",
        "        }\n",
        "\n",
        "    def cross_validate(self):\n",
        "        \"\"\"Perform cross-validation training.\"\"\"\n",
        "        logger.info(\"Starting cross-validation training...\")\n",
        "\n",
        "        # Prepare data\n",
        "        full_dataset, class_dist = self.prepare_data()\n",
        "\n",
        "        # Extract labels for stratification\n",
        "        labels = [sample[1] for sample in full_dataset.samples]\n",
        "\n",
        "        # Stratified K-Fold\n",
        "        skf = StratifiedKFold(n_splits=self.config['num_folds'], shuffle=True, random_state=42)\n",
        "\n",
        "        fold_results = []\n",
        "\n",
        "        for fold_num, (train_idx, val_idx) in enumerate(skf.split(range(len(full_dataset)), labels)):\n",
        "            # Create fold datasets\n",
        "            train_samples = [full_dataset.samples[i] for i in train_idx]\n",
        "            val_samples = [full_dataset.samples[i] for i in val_idx]\n",
        "\n",
        "            # Create fold-specific datasets\n",
        "            train_dataset = ImprovedPestDataset(\n",
        "                self.data_dir,\n",
        "                transform=self.augmentations.get_train_transforms(),\n",
        "                class_mapping={cls: idx for cls, idx in self.class_to_idx.items()}\n",
        "            )\n",
        "            train_dataset.samples = train_samples\n",
        "\n",
        "            val_dataset = ImprovedPestDataset(\n",
        "                self.data_dir,\n",
        "                transform=self.augmentations.get_val_transforms(),\n",
        "                class_mapping={cls: idx for cls, idx in self.class_to_idx.items()}\n",
        "            )\n",
        "            val_dataset.samples = val_samples\n",
        "\n",
        "            # Train fold\n",
        "            fold_result = self.train_fold(train_dataset, val_dataset, fold_num)\n",
        "            fold_results.append(fold_result)\n",
        "\n",
        "        # Calculate cross-validation metrics\n",
        "        cv_accuracies = [result['best_val_acc'] for result in fold_results]\n",
        "        mean_cv_acc = np.mean(cv_accuracies)\n",
        "        std_cv_acc = np.std(cv_accuracies)\n",
        "\n",
        "        logger.info(f\"Cross-validation complete!\")\n",
        "        logger.info(f\"Mean CV Accuracy: {mean_cv_acc:.2f}% ± {std_cv_acc:.2f}%\")\n",
        "        logger.info(f\"Individual fold accuracies: {cv_accuracies}\")\n",
        "\n",
        "        # Save results\n",
        "        cv_results = {\n",
        "            'mean_accuracy': mean_cv_acc,\n",
        "            'std_accuracy': std_cv_acc,\n",
        "            'fold_accuracies': cv_accuracies,\n",
        "            'config': self.config,\n",
        "            'class_mapping': {\n",
        "                'classes': self.class_names,\n",
        "                'class_to_idx': self.class_to_idx,\n",
        "                'num_classes': self.num_classes\n",
        "            }\n",
        "        }\n",
        "\n",
        "        with open(self.output_dir / 'cv_results.json', 'w') as f:\n",
        "            json.dump(cv_results, f, indent=2)\n",
        "\n",
        "        return cv_results, fold_results\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main training function.\"\"\"\n",
        "    # Set random seeds for reproducibility\n",
        "    random.seed(42)\n",
        "    np.random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(42)\n",
        "        torch.cuda.manual_seed_all(42)\n",
        "\n",
        "    print(\"Improved Pest Classification Training Pipeline\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Check dataset\n",
        "    data_dir = \"/content/datasets\"\n",
        "    if not Path(data_dir).exists():\n",
        "        print(f\"Dataset directory '{data_dir}' not found!\")\n",
        "        print(\"Please ensure the Agricultural Pests Image Dataset is available.\")\n",
        "        return\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = ImprovedTrainer(data_dir)\n",
        "\n",
        "    # Start training\n",
        "    start_time = time.time()\n",
        "    cv_results, fold_results = trainer.cross_validate()\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    print(f\"\\nTraining completed in {training_time:.2f} seconds!\")\n",
        "    print(f\"Final Results:\")\n",
        "    print(f\"   Mean CV Accuracy: {cv_results['mean_accuracy']:.2f}% ± {cv_results['std_accuracy']:.2f}%\")\n",
        "    print(f\"   Best Single Fold: {max(cv_results['fold_accuracies']):.2f}%\")\n",
        "    print(f\"   Models saved in: {trainer.output_dir}\")\n",
        "\n",
        "    return cv_results, fold_results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if not ML_AVAILABLE:\n",
        "        print(\"ML dependencies not available. Please install required packages.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    main()"
      ],
      "metadata": {
        "id": "QnNf91eY46Hc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "46bbb8ee-6657-474f-9ec4-fab09e3a72f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Improved Pest Classification Training Pipeline\n",
            "============================================================\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1036645291.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1036645291.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mcv_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m     \u001b[0mtraining_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1036645291.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0;31m# Train fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             \u001b[0mfold_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m             \u001b[0mfold_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1036645291.py\u001b[0m in \u001b[0;36mtrain_fold\u001b[0;34m(self, train_dataset, val_dataset, fold_num)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1036645291.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/efficientnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/efficientnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/efficientnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_res_connect\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstochastic_depth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \"\"\"\n\u001b[0;32m--> 193\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2820\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2822\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2823\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2824\u001b[0m         \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}